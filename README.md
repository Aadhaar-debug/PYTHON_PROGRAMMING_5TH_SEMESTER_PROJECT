
# AUTOMATING RETRO GAMES USING THE REINFROCEMENT LEARNING 
## TEAM NAME - TEAM Trex

- TEAM LEADER - AADHAAR KOUL (5th semester cse dept)
- TEAM MEMBER - ARJUN CHARAK (5th semester cse dept)
- TEAM MEMBER - SHOBIT KITCHLOO (5th semester cse dept)
- TEAM MEMBER - SIDDHARTH BHAWANI (5th semester cse dept)
- TEAM MEMBER - ANIL KUMAR (5th semester cse dept)

CLASS COORDINATOR - ARJUN PURI

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Complete Automation of some of the most renouned game titles using the reinforcement learning.

- EQUAL CONTRIBUTION
- NO PLAGIARISM
- COPYRIGHT RESERVED TO MIET JAMMU 5TH SEMESTER CLASS A1 STUDENTS

## ABSTRACT


General game testing relies on the use of
human play testers, play test scripting, and prior knowledge
of areas of interest to produce relevant test data. Using deep
reinforcement learning (DRL), we introduce a self-learning
mechanism to the game testing framework. With DRL, the
framework is capable of exploring and/or exploiting the
game mechanics based on a user-defined, reinforcing reward
signal. As a result, test coverage is increased and unintended
game play mechanics, exploits and bugs are discovered in
a multitude of game types. In this paper, we show that
DRL can be used to increase test coverage, find exploits,
test map difficulty, and to detect common problems that
arise in the testing of first-person shooter (FPS) games.In
this paper, we study applying Reinforcement Learning to
design a automatic agent to play the game Super Mario
Bros. One of the challenge is how to handle the complex
game environment. By abstracting the game environment
into a state vector and using Q learning — an algorithm
oblivious to transitional probabilities — we achieve tractable
computation time and fast convergence. After training
for 5000 iterations, our agent is able to win about 90
percent of the time. We also compare and analyze the choice
of different learning rate (alpha) and discount factor (gamma)


## Tech

- PYTHON - V3.10 , 3.8 AND 3.7
- PYTHON IDE

## Specifications (Minimum requirement)

- RAM - 2GB
- PROCESSOR - CORE 2 DUO OR HIGHER FOR Q LEARNING TABLE PLOTTING
- WINDOWS VERSION - 10 OR HIGHER
- PYTHON INTERPRETER VERSION - PYTHON 3.7 AND PYTHON 10.0
- INTERNET CONNECTIVITY - NOT REQUIRED 

NOTE : To install some of the packages the internet connectivity might be a requirement.

## IMPLEMENTATION

* DOWNLOAD THE CISCO PACKET TRACER  FROM THE NETCAD PORTAL
* DOUBLE TAP THE  ( .PKT ) FILE AND IT'LL AUTOMATICALLY OPEN UP THE FILE IN THE CISCO PACKET TRACER.


## DOCUMENT AND THE PPT FILE CAN BE FOUND IN THIS REPOSITORY ONLY JUST CHECK THE REPOSITORY CONTENTS ABOVE 

# SAMPLES / PICTURES 

![FRAMEWORK]([https://myoctocat.com/assets/images/base-octocat.svg](https://github.com/Aadhaar-debug/COMPUTER_NETWORKS_5th_SEMESTER_PROJECT-/blob/main/NETWORKING%20PROJECT%20PICTURES/NETWORK%20FRAMEWORK.jpg))
![PC CONFIGURATION]([https://myoctocat.com/assets/images/base-octocat.svg](https://github.com/Aadhaar-debug/COMPUTER_NETWORKS_5th_SEMESTER_PROJECT-/blob/main/NETWORKING%20PROJECT%20PICTURES/PC1%20CONFIGURATION%20FROM%20VLAN1.jpg))
![PING CAPTURE]([https://myoctocat.com/assets/images/base-octocat.svg](https://github.com/Aadhaar-debug/COMPUTER_NETWORKS_5th_SEMESTER_PROJECT-/blob/main/NETWORKING%20PROJECT%20PICTURES/PING%201%20PDU%20CAPTURE%20RESULT%20(PART%205).jpg))
![PING ENUMERATION]([https://myoctocat.com/assets/images/base-octocat.svg](https://github.com/Aadhaar-debug/COMPUTER_NETWORKS_5th_SEMESTER_PROJECT-/blob/main/NETWORKING%20PROJECT%20PICTURES/PING%201%20ROUTE%20NETWORK%20ENUMERATION%20(PART%205).jpg))
![FINAL RESULT]([https://myoctocat.com/assets/images/base-octocat.svg](https://github.com/Aadhaar-debug/COMPUTER_NETWORKS_5th_SEMESTER_PROJECT-/blob/main/NETWORKING%20PROJECT%20PICTURES/FINAL%20RESULT.jpg))

## License

MIT

**Free Software, Hell Yeah!**
